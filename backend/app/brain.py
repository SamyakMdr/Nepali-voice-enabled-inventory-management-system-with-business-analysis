import torch
import pickle
import re
import os
from transformers import AutoTokenizer, DistilBertForSequenceClassification

# -------------------------------------------------
# 1ï¸âƒ£ MODEL SETUP
# -------------------------------------------------

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
MODEL_PATH = os.path.join(BASE_DIR, "bert_brain_model")

print(f"ðŸ§  Loading SmartBiz Brain from: {MODEL_PATH}")

device = "cuda" if torch.cuda.is_available() else "cpu"
BERT_AVAILABLE = False
tokenizer = None
bert_model = None
id_to_label = {}

try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    bert_model = DistilBertForSequenceClassification.from_pretrained(MODEL_PATH)
    bert_model.to(device)
    bert_model.eval()
    torch.set_grad_enabled(False)

    with open(os.path.join(MODEL_PATH, "label_map.pkl"), "rb") as f:
        id_to_label = pickle.load(f)

    BERT_AVAILABLE = True
    print(f"âœ… BERT Loaded Successfully â€” labels: {list(id_to_label.values())}")

except Exception as e:
    print("âš ï¸ BERT Load Failed:", e)
    print("âš ï¸ Running in Rule-Based Mode Only")

# -------------------------------------------------
# 2ï¸âƒ£ KNOWN ITEMS (STRICT CONTROL)
# -------------------------------------------------

KNOWN_ITEMS = {
    "à¤šà¤¾à¤®à¤²", "à¤šà¤¾à¤®à¤²",
    "à¤¦à¤¾à¤²", "à¤®à¤¸à¥à¤°à¥‹", "à¤°à¤¹à¤°", "à¤®à¥à¤—à¥€", "à¤šà¤¨à¤¾",
    "à¤¤à¥‡à¤²", "à¤¸à¤¨à¤«à¥à¤²à¤¾à¤µà¤°",
    "à¤šà¤¿à¤¨à¥€",
    "à¤¨à¥à¤¨",
    "à¤šà¤¿à¤‰à¤°à¤¾",
    "à¤®à¥ˆà¤¦à¤¾",
    "à¤…à¤£à¥à¤¡à¤¾",
    "à¤¬à¥‡à¤¸à¤¾à¤°",
    "à¤¬à¤¿à¤¸à¥à¤•à¥à¤Ÿ",
}

# Map sub-items back to primary item names
ITEM_ALIASES = {
    "à¤®à¤¸à¥à¤°à¥‹": "à¤¦à¤¾à¤²",
    "à¤°à¤¹à¤°": "à¤¦à¤¾à¤²",
    "à¤®à¥à¤—à¥€": "à¤¦à¤¾à¤²",
    "à¤šà¤¨à¤¾": "à¤¦à¤¾à¤²",
    "à¤¸à¤¨à¤«à¥à¤²à¤¾à¤µà¤°": "à¤¤à¥‡à¤²",
    "à¤¬à¤¾à¤¸à¤®à¤¤à¥€": "à¤šà¤¾à¤®à¤²",
    "à¤œà¤¿à¤°à¤¾": "à¤šà¤¾à¤®à¤²",
    "à¤¸à¥‹à¤¨à¤¾": "à¤šà¤¾à¤®à¤²",
    "à¤®à¤¨à¥à¤¸à¥à¤²à¥€": "à¤šà¤¾à¤®à¤²",
    "à¤®à¤¸à¤¿à¤¨à¥‹": "à¤šà¤¾à¤®à¤²",
    # Common Whisper mishearings
    "à¤¤à¤¾à¤²": "à¤¦à¤¾à¤²",
    "à¤Ÿà¤¾à¤²": "à¤¦à¤¾à¤²",
    "à¤¥à¤¾à¤²": "à¤¦à¤¾à¤²",
    "à¤¦à¤¾à¤¨": "à¤¦à¤¾à¤²",
    "à¤œà¤®à¤¾à¤²": "à¤šà¤¾à¤®à¤²",
    "à¤¸à¤¾à¤®à¤²": "à¤šà¤¾à¤®à¤²",
    "à¤›à¤¾à¤®à¤²": "à¤šà¤¾à¤®à¤²",
    "à¤šà¤¿à¤¨à¤¿": "à¤šà¤¿à¤¨à¥€",
    "à¤›à¤¿à¤¨à¤¿": "à¤šà¤¿à¤¨à¥€",
    "à¤¸à¤¿à¤¨à¥€": "à¤šà¤¿à¤¨à¥€",
    "à¤Ÿà¥‡à¤²": "à¤¤à¥‡à¤²",
    "à¤Ÿà¥ˆà¤²": "à¤¤à¥‡à¤²",
    "à¤ªà¥‡à¤²": "à¤¤à¥‡à¤²",
    "à¤¨à¥‚à¤¨": "à¤¨à¥à¤¨",
    "à¤²à¥à¤¨": "à¤¨à¥à¤¨",
}

# -------------------------------------------------
# 3ï¸âƒ£ STRONG INTENT KEYWORDS (expanded)
# -------------------------------------------------

STRONG_KEYWORDS = {
    "SALE": [
        "à¤¬à¥‡à¤š", "à¤¬à¥‡à¤šà¥‡à¤‚", "à¤¬à¥‡à¤šà¤¿à¤¯à¥‹", "à¤¬à¤¿à¤•à¥à¤°à¥€",
        "à¤˜à¤Ÿà¤¾à¤‰", "à¤˜à¤Ÿà¤¾à¤Š", "à¤•à¤Ÿà¤¾à¤“", "à¤•à¤Ÿà¤¾à¤‰",
        "à¤¦à¥‡à¤‰", "à¤¦à¤¿à¤¨à¥", "à¤¦à¥‡ ", "à¤²à¤—à¥à¤¯à¥‹",
        "à¤¡à¥‡à¤²à¤¿à¤­à¤°à¥€", "à¤ªà¥à¤¯à¤¾à¤•", "à¤¦à¤°à¥à¤¤à¤¾",
    ],
    "ADD": [
        "à¤¥à¤ª", "à¤¥à¤ªà¤¿à¤¯à¥‹", "à¤°à¤¾à¤–", "à¤œà¥‹à¤¡",
        "à¤†à¤¯à¥‹", "à¤²à¥à¤¯à¤¾à¤Š", "à¤²à¥à¤¯à¤¾à¤‰", "à¤•à¤¿à¤¨à¥‡à¤°",
        "à¤…à¤ªà¤¡à¥‡à¤Ÿ", "à¤—à¥‹à¤¦à¤¾à¤®", "à¤¸à¥à¤Ÿà¤• à¤…à¤ªà¤¡à¥‡à¤Ÿ",
    ],
    "CHECK": [
        "à¤•à¤¤à¤¿", "à¤¬à¤¾à¤à¤•à¥€", "à¤¬à¤¾à¤‚à¤•à¥€",
        "à¤¹à¥‡à¤°", "à¤¸à¥à¤Ÿà¤•", "à¤¸à¤•à¤¿à¤¯à¥‹", "à¤¸à¤•à¤¿à¤¨",
        "à¤›", "à¤¹à¤¿à¤¸à¤¾à¤¬",
    ],
}

# -------------------------------------------------
# 4ï¸âƒ£ NEPALI NUMBER SUPPORT (expanded)
# -------------------------------------------------

NEPALI_NUMBERS = {
    "à¤à¤•": 1, "à¤¦à¥à¤ˆ": 2, "à¤¤à¥€à¤¨": 3, "à¤šà¤¾à¤°": 4, "à¤ªà¤¾à¤à¤š": 5,
    "à¤›": 6, "à¤¸à¤¾à¤¤": 7, "à¤†à¤ ": 8, "à¤¨à¥Œ": 9, "à¤¦à¤¶": 10,
    "à¤à¤˜à¤¾à¤°": 11, "à¤¬à¤¾à¤¹à¥à¤°": 12, "à¤¤à¥‡à¤¹à¥à¤°": 13, "à¤šà¥Œà¤§": 14,
    "à¤ªà¤¨à¥à¤§à¥à¤°": 15, "à¤¸à¥‹à¤¹à¥à¤°": 16, "à¤¸à¤¤à¥à¤°": 17, "à¤…à¤ à¤¾à¤°": 18,
    "à¤‰à¤¨à¥à¤¨à¤¾à¤‡à¤¸": 19, "à¤¬à¥€à¤¸": 20, "à¤ªà¤šà¥à¤šà¥€à¤¸": 25,
    "à¤¤à¥€à¤¸": 30, "à¤šà¤¾à¤²à¥€à¤¸": 40, "à¤ªà¤šà¤¾à¤¸": 50,
    "à¤¸à¤¾à¤ à¥€": 60, "à¤¸à¤¤à¥à¤¤à¤°à¥€": 70, "à¤…à¤¸à¥à¤¸à¥€": 80,
    "à¤¨à¤¬à¥à¤¬à¥‡": 90, "à¤¸à¤¯": 100,
    "à¤†à¤§à¤¾": 0.5, "à¤¡à¥‡à¤¢": 1.5, "à¤ªà¥Œà¤¨à¥‡": 0.75,
    "à¥§": 1, "à¥¨": 2, "à¥©": 3, "à¥ª": 4, "à¥«": 5,
    "à¥¬": 6, "à¥­": 7, "à¥®": 8, "à¥¯": 9, "à¥§à¥¦": 10,
    "à¥¨à¥¦": 20, "à¥¨à¥«": 25, "à¥©à¥¦": 30, "à¥«à¥¦": 50, "à¥§à¥¦à¥¦": 100,
}

def nepali_num_to_english(text):
    """Convert Nepali digits to English digits."""
    mapping = str.maketrans("à¥¦à¥§à¥¨à¥©à¥ªà¥«à¥¬à¥­à¥®à¥¯", "0123456789")
    return text.translate(mapping)

# -------------------------------------------------
# 5ï¸âƒ£ NORMALIZATION
# -------------------------------------------------

def normalize_nepali(text):
    """Normalize Nepali text for fuzzy matching (exported for main.py)."""
    text = text.strip()
    replacements = {
        "à¤•à¥à¤·": "à¤›",
        "à¤¶": "à¤¸",
        "à¤·": "à¤¸",
        "à¤µ": "à¤¬",
        "à¤£": "à¤¨",
        "à¥ˆ": "à¥‡",
        "à¥Œ": "à¥‹",
    }
    for k, v in replacements.items():
        text = text.replace(k, v)
    return text


def normalize_text(text):
    """Full normalization: lowercase + phonetic folding."""
    return normalize_nepali(text.lower().strip())

# -------------------------------------------------
# 6ï¸âƒ£ EXTRACT QUANTITY + UNIT
# -------------------------------------------------

def extract_quantity_and_unit(text):
    text_en = nepali_num_to_english(text)

    # 1) Try Nepali word numbers first (longest match first)
    quantity = None
    for word, val in sorted(NEPALI_NUMBERS.items(), key=lambda x: -len(x[0])):
        if word in text:
            quantity = val
            break

    # 2) Fall back to digit regex
    if quantity is None:
        match = re.search(r"\d+(\.\d+)?", text_en)
        quantity = float(match.group()) if match else 1.0

    # Detect unit
    if "à¤•à¤¿à¤²à¥‹" in text or "kg" in text.lower():
        unit = "kg"
    elif "à¤¬à¥‹à¤°à¤¾" in text:
        unit = "bora"
    elif "à¤µà¤Ÿà¤¾" in text:
        unit = "piece"
    elif "à¤ªà¥à¤¯à¤¾à¤•à¥‡à¤Ÿ" in text:
        unit = "packet"
    elif "à¤²à¤¿à¤Ÿà¤°" in text:
        unit = "litre"
    elif "à¤•à¤¾à¤°à¥à¤Ÿà¥à¤¨" in text:
        unit = "carton"
    else:
        unit = "kg"

    return float(quantity), unit

# -------------------------------------------------
# 7ï¸âƒ£ EXTRACT ITEM (with aliases + substring matching)
# -------------------------------------------------

def extract_item(text):
    words = text.split()

    # Direct match
    for word in words:
        if word in KNOWN_ITEMS:
            return word

    # Alias match
    for word in words:
        if word in ITEM_ALIASES:
            return ITEM_ALIASES[word]

    # Substring match (e.g. "à¤šà¤¾à¤®à¤²à¤•à¥‹" contains "à¤šà¤¾à¤®à¤²")
    for item in KNOWN_ITEMS:
        if item in text:
            return item

    return None

# -------------------------------------------------
# 8ï¸âƒ£ BERT INFERENCE
# -------------------------------------------------

def predict_intent_bert(text, threshold=0.55):
    """Run BERT inference and return (intent, confidence) or (None, 0)."""
    if not BERT_AVAILABLE:
        return None, 0.0

    try:
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=64,
        ).to(device)

        with torch.no_grad():
            outputs = bert_model(**inputs)

        probs = torch.softmax(outputs.logits, dim=1)
        confidence, pred_id = torch.max(probs, dim=1)
        confidence = confidence.item()
        pred_id = pred_id.item()
        intent = id_to_label.get(pred_id, "UNKNOWN")

        print(f"ðŸ¤– BERT â†’ {intent} ({confidence:.2f})")
        return intent, confidence

    except Exception as e:
        print("âŒ BERT Error:", e)
        return None, 0.0

# -------------------------------------------------
# 9ï¸âƒ£ RULE-BASED INTENT
# -------------------------------------------------

def predict_intent_rules(text):
    """Return intent from keyword matching, or None."""
    for category, keywords in STRONG_KEYWORDS.items():
        for keyword in keywords:
            if keyword in text:
                print(f"âš¡ Rule Match: {category} (keyword: {keyword})")
                return category
    return None

# -------------------------------------------------
# ðŸ”Ÿ MAIN PROCESS FUNCTION
# -------------------------------------------------

def process_command_with_ai(text):
    print(f"\nðŸ§  Processing: {text}")

    text_clean = normalize_text(text)
    # Also keep original (un-normalized) for BERT â€” the model was trained on raw text
    text_original = text.strip()

    # ----------------------------
    # A. RULE-BASED PRIORITY
    # ----------------------------
    intent = predict_intent_rules(text_clean)

    # ----------------------------
    # B. BERT (on original text â€” matches training data better)
    # ----------------------------
    if intent is None:
        bert_intent, bert_conf = predict_intent_bert(text_original)
        if bert_intent and bert_conf >= 0.55:
            intent = bert_intent

    # ----------------------------
    # C. BERT on normalized text as secondary attempt
    # ----------------------------
    if intent is None:
        bert_intent, bert_conf = predict_intent_bert(text_clean)
        if bert_intent and bert_conf >= 0.50:
            intent = bert_intent

    if intent is None:
        intent = "UNKNOWN"

    # ----------------------------
    # D. EXTRACT DATA
    # ----------------------------
    item = extract_item(text_clean) or extract_item(text_original)
    quantity, unit = extract_quantity_and_unit(text_original)

    print(f"ðŸ“‹ Result: intent={intent}, item={item}, qty={quantity}, unit={unit}")

    return {
        "intent": intent,
        "item": item,
        "quantity": quantity,
        "unit": unit,
        "customer": None,
    }
